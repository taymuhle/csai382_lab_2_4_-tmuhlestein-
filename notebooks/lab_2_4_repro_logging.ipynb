{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f5a07ee-e2ac-42d5-918c-addb4321f6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Find repo root (the folder that contains README.md and RUN.md)\n",
    "here = Path.cwd()\n",
    "repo_root = None\n",
    "for p in [here] + list(here.parents):\n",
    "    if (p / \"README.md\").exists() and (p / \"RUN.md\").exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "\n",
    "print(\"cwd:\", here)\n",
    "print(\"repo_root:\", repo_root)\n",
    "\n",
    "if repo_root is None:\n",
    "    raise FileNotFoundError(\"Could not locate repo root (README.md and RUN.md not found above cwd).\")\n",
    "\n",
    "# Paths first (so logging can reference them)\n",
    "data_dir = repo_root / \"data_samples\"   # keep your current choice for now\n",
    "menu_path = data_dir / \"menu_items.csv\"\n",
    "orders_path = data_dir / \"order_details.csv\"\n",
    "\n",
    "print(\"data_dir:\", data_dir)\n",
    "print(\"data_dir exists:\", data_dir.exists())\n",
    "if data_dir.exists():\n",
    "    print(\"files in data_samples:\", [x.name for x in data_dir.iterdir()])\n",
    "print(\"menu_path:\", menu_path, \"exists:\", menu_path.exists())\n",
    "print(\"orders_path:\", orders_path, \"exists:\", orders_path.exists())\n",
    "\n",
    "# ---- Part B: Logging setup (after variables exist) ----\n",
    "import logging, sys\n",
    "from datetime import datetime\n",
    "import platform, socket\n",
    "\n",
    "logs_dir = repo_root / \"logs\"\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "log_path = logs_dir / f\"run_{run_ts}.log\"\n",
    "\n",
    "logger = logging.getLogger(\"lab_2_4\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "if logger.handlers:\n",
    "    logger.handlers.clear()\n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "console = logging.StreamHandler(sys.stdout)\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(fmt)\n",
    "\n",
    "fileh = logging.FileHandler(log_path, mode=\"a\", encoding=\"utf-8\")\n",
    "fileh.setLevel(logging.INFO)\n",
    "fileh.setFormatter(fmt)\n",
    "\n",
    "logger.addHandler(console)\n",
    "logger.addHandler(fileh)\n",
    "\n",
    "logger.info(\"Run started\")\n",
    "logger.info(f\"repo_root={repo_root}\")\n",
    "logger.info(f\"log_path={log_path}\")\n",
    "logger.info(f\"Python={sys.version.split()[0]}\")\n",
    "logger.info(f\"Platform={platform.platform()}\")\n",
    "logger.info(f\"Host={socket.gethostname()}\")\n",
    "logger.info(f\"data_dir={data_dir}\")\n",
    "logger.info(f\"menu_path={menu_path}\")\n",
    "logger.info(f\"orders_path={orders_path}\")\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Load CSVs\n",
    "menu_items = pd.read_csv(menu_path)\n",
    "order_details = pd.read_csv(orders_path)\n",
    "\n",
    "logger.info(f\"Loaded menu_items: {menu_items.shape}\")\n",
    "logger.info(f\"Loaded order_details: {order_details.shape}\")\n",
    "\n",
    "menu_items.head(), order_details.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e74626-3b51-4bb9-869c-d052b97b6c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, random, json, hashlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# seeds\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "logger.info(\"Reproducibility: seeds set (PYTHONHASHSEED=0, random=0, numpy=0)\")\n",
    "\n",
    "# sha256 for the two input CSVs\n",
    "def sha256_file(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "inputs = [menu_path, orders_path]\n",
    "hashes = {p.name: sha256_file(p) for p in inputs}\n",
    "\n",
    "for name, h in hashes.items():\n",
    "    logger.info(f\"SHA-256 {name}: {h}\")\n",
    "\n",
    "hash_path = repo_root / \"data_hashes.json\"\n",
    "hash_path.write_text(json.dumps(hashes, indent=2), encoding=\"utf-8\")\n",
    "logger.info(f\"Wrote data hashes to {hash_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec54703-8b1e-4345-b4d1-8d6a263a0ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04a59a6-60e8-4eb3-8cc8-3805e93f6c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "src = Path.cwd() / \"requirements.txt\"\n",
    "dst = repo_root / \"requirements.txt\"\n",
    "\n",
    "print(\"wrote here:\", src, \"exists:\", src.exists())\n",
    "\n",
    "if not src.exists():\n",
    "    raise FileNotFoundError(\"requirements.txt wasn't created in the notebook CWD. If %pip ran, Databricks wrote it elsewhere.\")\n",
    "\n",
    "dst.write_text(src.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "logger.info(f\"Wrote requirements to {dst}\")\n",
    "print(\"repo requirements:\", dst, \"exists:\", dst.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebcc17b4-df98-475f-b98f-5cde167f85bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(f\"menu_items columns: {list(menu_items.columns)}\")\n",
    "logger.info(f\"order_details columns: {list(order_details.columns)}\")\n",
    "\n",
    "print(\"menu_items columns:\", list(menu_items.columns))\n",
    "print(\"order_details columns:\", list(order_details.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7773fcc7-1771-4f52-934d-25bf876b8268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# clean text columns (trim)\n",
    "for df in (menu_items, order_details):\n",
    "    for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# enforce types\n",
    "menu_items[\"menu_item_id\"] = pd.to_numeric(menu_items[\"menu_item_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "menu_items[\"price\"] = pd.to_numeric(menu_items[\"price\"], errors=\"coerce\")\n",
    "\n",
    "order_details[\"item_id\"] = pd.to_numeric(order_details[\"item_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "order_details[\"order_id\"] = pd.to_numeric(order_details[\"order_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "order_details[\"order_details_id\"] = pd.to_numeric(order_details[\"order_details_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# build timestamp from date + time\n",
    "order_details[\"order_ts\"] = pd.to_datetime(\n",
    "    order_details[\"order_date\"].astype(str) + \" \" + order_details[\"order_time\"].astype(str),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "logger.info(\"Cleaned types and created order_ts\")\n",
    "\n",
    "# join\n",
    "joined = order_details.merge(\n",
    "    menu_items,\n",
    "    left_on=\"item_id\",\n",
    "    right_on=\"menu_item_id\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Joined shape: {joined.shape}\")\n",
    "\n",
    "# tidy table\n",
    "tidy = joined[[\"order_id\", \"order_ts\", \"item_name\", \"category\", \"price\"]].copy()\n",
    "tidy[\"quantity\"] = 1  # each row is one item in this dataset\n",
    "tidy[\"line_total\"] = tidy[\"price\"] * tidy[\"quantity\"]\n",
    "\n",
    "logger.info(f\"Tidy shape: {tidy.shape}\")\n",
    "tidy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376f2842-89a3-421b-aade-853ad7022dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# metrics\n",
    "top5_items = (\n",
    "    tidy.groupby(\"item_name\")[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(5)\n",
    "    .reset_index(name=\"total_quantity\")\n",
    ")\n",
    "\n",
    "revenue_by_category = (\n",
    "    tidy.groupby(\"category\")[\"line_total\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .reset_index(name=\"revenue\")\n",
    ")\n",
    "\n",
    "busiest_hour = (\n",
    "    tidy.dropna(subset=[\"order_ts\"])\n",
    "        .assign(hour=tidy[\"order_ts\"].dt.hour)\n",
    "        .groupby(\"hour\")[\"quantity\"]\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(1)\n",
    "        .reset_index(name=\"items_sold\")\n",
    ")\n",
    "\n",
    "logger.info(\"Computed metrics: top5_items, revenue_by_category, busiest_hour\")\n",
    "\n",
    "display(top5_items)\n",
    "display(revenue_by_category)\n",
    "display(busiest_hour)\n",
    "\n",
    "# save combined metrics file (simple tidy format)\n",
    "metrics_rows = []\n",
    "\n",
    "for _, r in top5_items.iterrows():\n",
    "    metrics_rows.append({\"metric\": \"top_item_by_quantity\", \"dimension\": r[\"item_name\"], \"value\": int(r[\"total_quantity\"])})\n",
    "\n",
    "for _, r in revenue_by_category.iterrows():\n",
    "    metrics_rows.append({\"metric\": \"revenue_by_category\", \"dimension\": r[\"category\"], \"value\": float(r[\"revenue\"])})\n",
    "\n",
    "if len(busiest_hour) == 1:\n",
    "    metrics_rows.append({\"metric\": \"busiest_hour\", \"dimension\": str(int(busiest_hour.loc[0, \"hour\"])), \"value\": int(busiest_hour.loc[0, \"items_sold\"])})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "\n",
    "out_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_dir_dbfs = \"/dbfs/FileStore/tables/etl_output\"\n",
    "os.makedirs(out_dir_dbfs, exist_ok=True)\n",
    "\n",
    "metrics_out_path = f\"{out_dir_dbfs}/metrics_{out_ts}.csv\"\n",
    "metrics_df.to_csv(metrics_out_path, index=False)\n",
    "\n",
    "logger.info(f\"Wrote metrics CSV: {metrics_out_path}\")\n",
    "print(\"Browser path:\", f\"/files/tables/etl_output/metrics_{out_ts}.csv\")\n",
    "\n",
    "metrics_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2240317e-3718-4993-aa35-967e46592853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# make sure the DBFS folder exists\n",
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/tables/etl_output\")\n",
    "\n",
    "# build metrics_df again (in case the previous cell stopped early)\n",
    "top5_items = (\n",
    "    tidy.groupby(\"item_name\")[\"quantity\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(5)\n",
    "    .reset_index(name=\"total_quantity\")\n",
    ")\n",
    "\n",
    "revenue_by_category = (\n",
    "    tidy.groupby(\"category\")[\"line_total\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .reset_index(name=\"revenue\")\n",
    ")\n",
    "\n",
    "busiest_hour = (\n",
    "    tidy.dropna(subset=[\"order_ts\"])\n",
    "        .assign(hour=tidy[\"order_ts\"].dt.hour)\n",
    "        .groupby(\"hour\")[\"quantity\"]\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(1)\n",
    "        .reset_index(name=\"items_sold\")\n",
    ")\n",
    "\n",
    "metrics_rows = []\n",
    "\n",
    "for _, r in top5_items.iterrows():\n",
    "    metrics_rows.append({\"metric\": \"top_item_by_quantity\", \"dimension\": r[\"item_name\"], \"value\": int(r[\"total_quantity\"])})\n",
    "\n",
    "for _, r in revenue_by_category.iterrows():\n",
    "    metrics_rows.append({\"metric\": \"revenue_by_category\", \"dimension\": r[\"category\"], \"value\": float(r[\"revenue\"])})\n",
    "\n",
    "if len(busiest_hour) == 1:\n",
    "    metrics_rows.append({\"metric\": \"busiest_hour\", \"dimension\": str(int(busiest_hour.loc[0, \"hour\"])), \"value\": int(busiest_hour.loc[0, \"items_sold\"])})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "\n",
    "# write locally, then copy to dbfs:/FileStore/...\n",
    "out_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "local_tmp = f\"/tmp/metrics_{out_ts}.csv\"\n",
    "dbfs_path = f\"dbfs:/FileStore/tables/etl_output/metrics_{out_ts}.csv\"\n",
    "\n",
    "metrics_df.to_csv(local_tmp, index=False)\n",
    "dbutils.fs.cp(f\"file:{local_tmp}\", dbfs_path)\n",
    "\n",
    "logger.info(f\"Wrote metrics CSV to {dbfs_path}\")\n",
    "print(\"Browser path:\", f\"/files/tables/etl_output/metrics_{out_ts}.csv\")\n",
    "\n",
    "metrics_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55109bfe-5dad-4e61-a3d6-c3d212192202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "out_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "out_dir = repo_root / \"etl_pipeline\" / \"etl_output\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_repo_path = out_dir / f\"metrics_{out_ts}.csv\"\n",
    "metrics_df.to_csv(metrics_repo_path, index=False)\n",
    "\n",
    "logger.info(f\"Wrote metrics CSV to repo: {metrics_repo_path}\")\n",
    "print(\"metrics saved to:\", metrics_repo_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b84431c-2870-43d3-8f7c-f6a9782a1d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_details[\"order_ts\"] = pd.to_datetime(\n",
    "    order_details[\"order_date\"].astype(str) + \" \" + order_details[\"order_time\"].astype(str),\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "logger.info(\"Rebuilt order_ts using explicit format %Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e650a6e9-56e3-444b-b7e8-9f94ed747ad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assert: joined has no missing menu info for normal rows (it should match almost all)\n",
    "assert joined[\"item_name\"].notna().any(), \"Join failed: item_name is all null\"\n",
    "\n",
    "# Assert: tidy expected columns exist\n",
    "expected_cols = [\"order_id\", \"order_ts\", \"item_name\", \"category\", \"price\", \"quantity\", \"line_total\"]\n",
    "missing = [c for c in expected_cols if c not in tidy.columns]\n",
    "assert not missing, f\"Tidy missing columns: {missing}\"\n",
    "\n",
    "# Assert: tidy and metrics not empty\n",
    "assert len(tidy) > 0, \"Tidy output is empty\"\n",
    "assert len(metrics_df) > 0, \"Metrics output is empty\"\n",
    "\n",
    "logger.info(\"Assert checks passed\")\n",
    "print(\"asserts passed\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_4_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
