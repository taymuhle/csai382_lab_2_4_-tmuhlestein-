{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d98f6f-09fc-4fc4-a32a-0e7ca158399c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading the dataset"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_spark = spark.table(\"workspace.silver.labeled_step_test\")\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52719f24-5d46-4876-86f1-9cf3ae205ebf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Confirm dataset columns"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8f3c40-507e-4510-84b8-ef9d1d28a607",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check label distribution"
    }
   },
   "outputs": [],
   "source": [
    "df[\"step_label\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be982da8-f62b-424d-9c60-4a686fa38ea3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define feature and label columns"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric feature columns (already numbers)\n",
    "feature_cols_numeric = [\"distance_cm\"]\n",
    "\n",
    "# Categorical feature columns (text/IDs that need encoding)\n",
    "feature_cols_categorical = [\"sensorType\", \"deviceId\"]\n",
    "\n",
    "# Label column (what we want to predict)\n",
    "label_col = \"step_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb942945-af31-492c-af1a-2e55dc87fcf0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a Train/Test Split pt 1"
    }
   },
   "outputs": [],
   "source": [
    "# See the exact column names that exist in your dataframe\n",
    "print(df.columns)\n",
    "\n",
    "# Helpful: show anything that looks like sensor/device/step/distance\n",
    "print(\"\\nPossible sensor columns:\", [c for c in df.columns if \"sensor\" in c.lower()])\n",
    "print(\"Possible device columns:\", [c for c in df.columns if \"device\" in c.lower()])\n",
    "print(\"Possible label columns:\",  [c for c in df.columns if \"step\" in c.lower() and \"label\" in c.lower()])\n",
    "print(\"Possible distance columns:\", [c for c in df.columns if \"dist\" in c.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e3a5f0-8efc-462b-8fa0-5594ad2fd271",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a Train/Test Split pt 2"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[feature_cols_numeric + feature_cols_categorical]\n",
    "y = df[label_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"\\nTrain label distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest label distribution:\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd6e345-1a6c-46bd-8218-679fe9f2a9c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build Preprocessing Steps"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Scale numeric features\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Combine transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_cols_numeric),\n",
    "        (\"cat\", categorical_transformer, feature_cols_categorical)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d617b8-16f6-4da1-86ed-c3d69462922d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build the pipeline"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor)\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3739a9-57d5-4e4e-80a7-2d7b81790fcf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fit + transform"
    }
   },
   "outputs": [],
   "source": [
    "# Fit only on training data (avoids leakage)\n",
    "pipeline.fit(X_train)\n",
    "\n",
    "# Transform both sets\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "print(\"Transformed train shape:\", X_train_transformed.shape)\n",
    "print(\"Transformed test shape:\", X_test_transformed.shape)\n",
    "print(\"Type:\", type(X_train_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f53a9a-d75d-48b1-860f-ce03ac5f5e99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save the pipeline"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Option A: always writable on the cluster node\n",
    "pipeline_path = \"/tmp/stedi_feature_pipeline.pkl\"\n",
    "joblib.dump(pipeline, pipeline_path)\n",
    "\n",
    "print(\"Saved pipeline to:\", pipeline_path)\n",
    "print(\"File exists:\", os.path.exists(pipeline_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcde90af-7b48-4373-8758-96c12f04497f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "export_dir = \"/tmp/exports/model\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(pipeline, f\"{export_dir}/stedi_feature_pipeline.pkl\")\n",
    "\n",
    "# Save transformed features\n",
    "np.save(f\"{export_dir}/X_train_transformed.npy\", X_train_transformed, allow_pickle=True)\n",
    "np.save(f\"{export_dir}/X_test_transformed.npy\", X_test_transformed, allow_pickle=True)\n",
    "\n",
    "# Save labels\n",
    "pd.to_pickle(y_train, f\"{export_dir}/y_train.pkl\")\n",
    "pd.to_pickle(y_test, f\"{export_dir}/y_test.pkl\")\n",
    "\n",
    "print(\"Saved files to:\", export_dir)\n",
    "print(os.listdir(export_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2639966-5daa-4d07-9dd9-ed7444da1e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Markdown\n",
    "A consistent feature pipeline helps prevent unfairness because every row of data is processed the same way, every time. That reduces hidden bias caused by inconsistent scaling or encoding and makes the results easier to audit. It also helps avoid data leakage by fitting preprocessing only on the training set. Doctrine and Covenants 130:20â€“21 reminds me that reliable outcomes come from consistent principles, which connects to fairness in how we apply rules to everyone."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "part3_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
