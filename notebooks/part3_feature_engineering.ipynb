{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d98f6f-09fc-4fc4-a32a-0e7ca158399c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading the dataset"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_spark = spark.table(\"workspace.silver.labeled_step_test\")\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52719f24-5d46-4876-86f1-9cf3ae205ebf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Confirm dataset columns"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8f3c40-507e-4510-84b8-ef9d1d28a607",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check label distribution"
    }
   },
   "outputs": [],
   "source": [
    "df[\"step_label\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be982da8-f62b-424d-9c60-4a686fa38ea3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define feature and label columns"
    }
   },
   "outputs": [],
   "source": [
    "# Numeric feature columns (already numbers)\n",
    "feature_cols_numeric = [\"distance_cm\"]\n",
    "\n",
    "# Categorical feature columns (text/IDs that need encoding)\n",
    "feature_cols_categorical = [\"sensorType\", \"deviceId\"]\n",
    "\n",
    "# Label column (what we want to predict)\n",
    "label_col = \"step_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb942945-af31-492c-af1a-2e55dc87fcf0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a Train/Test Split pt 1"
    }
   },
   "outputs": [],
   "source": [
    "# See the exact column names that exist in your dataframe\n",
    "print(df.columns)\n",
    "\n",
    "# Helpful: show anything that looks like sensor/device/step/distance\n",
    "print(\"\\nPossible sensor columns:\", [c for c in df.columns if \"sensor\" in c.lower()])\n",
    "print(\"Possible device columns:\", [c for c in df.columns if \"device\" in c.lower()])\n",
    "print(\"Possible label columns:\",  [c for c in df.columns if \"step\" in c.lower() and \"label\" in c.lower()])\n",
    "print(\"Possible distance columns:\", [c for c in df.columns if \"dist\" in c.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e3a5f0-8efc-462b-8fa0-5594ad2fd271",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a Train/Test Split pt 2"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use correct column names from df\n",
    "feature_cols_numeric = [\"distance_cm\"]\n",
    "feature_cols_categorical = [\"sensor_type\", \"device_id\"]\n",
    "label_col = \"step_label\"\n",
    "\n",
    "X = df[feature_cols_numeric + feature_cols_categorical]\n",
    "y = df[label_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"\\nTrain label distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest label distribution:\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd6e345-1a6c-46bd-8218-679fe9f2a9c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build Preprocessing Steps"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Scale numeric features\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Combine transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_cols_numeric),\n",
    "        (\"cat\", categorical_transformer, feature_cols_categorical)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d617b8-16f6-4da1-86ed-c3d69462922d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build the pipeline"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor)\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3739a9-57d5-4e4e-80a7-2d7b81790fcf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fit + transform"
    }
   },
   "outputs": [],
   "source": [
    "# Fit only on training data (avoids leakage)\n",
    "pipeline.fit(X_train)\n",
    "\n",
    "# Transform both sets\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "print(\"Transformed train shape:\", X_train_transformed.shape)\n",
    "print(\"Transformed test shape:\", X_test_transformed.shape)\n",
    "print(\"Type:\", type(X_train_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca60618d-661d-4235-a3d1-6ed578b00242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for v in [\"pipeline\", \"X_train_transformed\", \"X_test_transformed\", \"y_train\", \"y_test\"]:\n",
    "    print(v, \"->\", \"FOUND\" if v in locals() else \"missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12190e95-ef92-4380-98bc-ed51fa3fa325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, joblib\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())  # one level up from /notebooks\n",
    "save_dir = os.path.join(project_root, \"etl_pipeline\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(pipeline, os.path.join(save_dir, \"stedi_feature_pipeline.pkl\"))\n",
    "joblib.dump(X_train_transformed, os.path.join(save_dir, \"X_train_transformed.pkl\"))\n",
    "joblib.dump(X_test_transformed,  os.path.join(save_dir, \"X_test_transformed.pkl\"))\n",
    "joblib.dump(y_train, os.path.join(save_dir, \"y_train.pkl\"))\n",
    "joblib.dump(y_test,  os.path.join(save_dir, \"y_test.pkl\"))\n",
    "\n",
    "print(\"Saved to:\", save_dir)\n",
    "print(\"Now contains:\", os.listdir(save_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f53a9a-d75d-48b1-860f-ce03ac5f5e99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save the pipeline"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Option A: always writable on the cluster node\n",
    "pipeline_path = \"/tmp/stedi_feature_pipeline.pkl\"\n",
    "joblib.dump(pipeline, pipeline_path)\n",
    "\n",
    "print(\"Saved pipeline to:\", pipeline_path)\n",
    "print(\"File exists:\", os.path.exists(pipeline_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcde90af-7b48-4373-8758-96c12f04497f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "export_dir = \"/tmp/exports/model\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(pipeline, f\"{export_dir}/stedi_feature_pipeline.pkl\")\n",
    "\n",
    "# Save transformed features\n",
    "np.save(f\"{export_dir}/X_train_transformed.npy\", X_train_transformed, allow_pickle=True)\n",
    "np.save(f\"{export_dir}/X_test_transformed.npy\", X_test_transformed, allow_pickle=True)\n",
    "\n",
    "# Save labels\n",
    "pd.to_pickle(y_train, f\"{export_dir}/y_train.pkl\")\n",
    "pd.to_pickle(y_test, f\"{export_dir}/y_test.pkl\")\n",
    "\n",
    "print(\"Saved files to:\", export_dir)\n",
    "print(os.listdir(export_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf86cc7-a628-4b95-a026-843585d96bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime, to_timestamp\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql(\"USE silver\")\n",
    "\n",
    "df_silver = spark.table(\"labeled_step_test\")\n",
    "df_silver.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbbaae54-6d7b-441e-b7d2-844a7c9fa3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fixed = (\n",
    "    df_silver\n",
    "    .withColumn(\"timestamp_ts\", to_timestamp(from_unixtime(col(\"timestamp\") / 1000)))\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_ms\")\n",
    "    .withColumnRenamed(\"timestamp_ts\", \"timestamp\")\n",
    "    .withColumnRenamed(\"source\", \"source_label\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19cd2353-2121-4f65-a9fc-1759d79620f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE silver\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW labeled_step_test_fixed AS\n",
    "SELECT\n",
    "  to_timestamp(from_unixtime(timestamp / 1000)) AS timestamp,   -- readable datetime\n",
    "  CAST(timestamp AS BIGINT) AS timestamp_ms,                    -- original epoch ms\n",
    "  device_id,\n",
    "  sensor_type,\n",
    "  distance_cm,\n",
    "  step_label,\n",
    "  source AS source_label\n",
    "FROM labeled_step_test\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed4ce3c-fd56-4fb1-a19e-f6fcd5dc6fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE silver\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW labeled_step_test_fixed AS\n",
    "SELECT\n",
    "  to_timestamp(from_unixtime(\n",
    "    CAST(COALESCE(timestamp_ms, CAST(timestamp AS BIGINT)) AS BIGINT) / 1000\n",
    "  )) AS timestamp,\n",
    "  device_id,\n",
    "  sensor_type,\n",
    "  distance_cm,\n",
    "  step_label,\n",
    "  source_label\n",
    "FROM labeled_step_test\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26644a8-22d9-406e-8e57-51f2ef92d832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE silver.labeled_step_test AS\n",
    "SELECT * FROM labeled_step_test_fixed\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe95f9ef-63cc-4096-961e-b80476c5ad9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"silver.labeled_step_test\").printSchema()\n",
    "display(spark.table(\"silver.labeled_step_test\").limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2639966-5daa-4d07-9dd9-ed7444da1e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Markdown\n",
    "A consistent feature pipeline helps prevent unfairness because every row of data is processed the same way, every time. That reduces hidden bias caused by inconsistent scaling or encoding and makes the results easier to audit. It also helps avoid data leakage by fitting preprocessing only on the training set. Doctrine and Covenants 130:20â€“21 reminds me that reliable outcomes come from consistent principles, which connects to fairness in how we apply rules to everyone."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "part3_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
